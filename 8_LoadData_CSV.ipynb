{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf.dataを使ってCSVをロードする\n",
    "\n",
    "CSVデータを`tf.data.Dataset`にロードする方法\n",
    "\n",
    "このチュートリアルでは、タイタニック号の乗客リストから取られたデータを使用する。\n",
    "乗客が生き残る可能性を、年齢、性別、チケットの等級、そして乗客が一人で旅行しているか否かといった特性から予測することを試みる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "TRAIN_DATA_URL = \"https://storage.googleapis.com/tf-datasets/titanic/train.csv\"\n",
    "TEST_DATA_URL = \"https://storage.googleapis.com/tf-datasets/titanic/eval.csv\"\n",
    "\n",
    "train_file_path = tf.keras.utils.get_file(\"train.csv\", TRAIN_DATA_URL)\n",
    "test_file_path = tf.keras.utils.get_file(\"eval.csv\", TEST_DATA_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numpyの値を読みやすくする\n",
    "np.set_printoptions(precision=3, suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データのロード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "'head' �́A�����R�}���h�܂��͊O���R�}���h�A\n����\\�ȃv���O�����܂��̓o�b�` �t�@�C���Ƃ��ĔF������Ă��܂���B\n"
    }
   ],
   "source": [
    "!head {train_file_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['survived', 'sex', 'age', 'n_siblings_spouses', 'parch', 'fare', 'class', 'deck', 'embark_town', 'alone']\n"
    }
   ],
   "source": [
    "# 入力ファイル中のCSV列\n",
    "with open(train_file_path, 'r') as f:\n",
    "    names_row = f.readline()\n",
    "\n",
    "CSV_COLUMNS = names_row.rstrip('\\n').split(',')\n",
    "print(CSV_COLUMNS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データセットコンストラクタはこれらのラベルを自動的にピックアップする。\n",
    "\n",
    "使用するファイルの1行目に列名がない場合、`make_csv_dataset`関数の`column_names`引数に文字列のリストとして渡すこと。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "SyntaxError",
     "evalue": "positional argument follows keyword argument (<ipython-input-34-3ebdb4af0a80>, line 6)",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-34-3ebdb4af0a80>\"\u001b[1;36m, line \u001b[1;32m6\u001b[0m\n\u001b[1;33m    ...)\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m positional argument follows keyword argument\n"
     ]
    }
   ],
   "source": [
    "CSV_COLUMNS = ['survived', 'sex', 'age', 'n_siblings_spouses', 'parch', 'fare', 'class', 'deck', 'embark_town', 'alone']\n",
    "\n",
    "dataset = tf.data.experimental.make_csv_dataset(\n",
    "     ...,\n",
    "     column_names=CSV_COLUMNS,\n",
    "     ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "もしデータセットから一部の列を除きたい場合には、使用したい列だけを含むリストを作り、コンストラクタのオプションである`select_columns` 引数として渡すこと。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-35-2604f64009a5>, line 7)",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-35-2604f64009a5>\"\u001b[1;36m, line \u001b[1;32m7\u001b[0m\n\u001b[1;33m    ...\u001b[0m\n\u001b[1;37m      ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "drop_columns = [\"fare\", \"embark_town\"]\n",
    "columns_to_use = [col for col in CSV_COLUMNS if col not in drop_columns]\n",
    "\n",
    "dataset = tf.data.experimental.make_csv_dataset(\n",
    "    ...,\n",
    "    select_columns=columns_to_use\n",
    "    ...\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "各サンプルのラベルとなる列を特定し、それが何であるかを示す必要がある。\n",
    "つまり、モデルに推論させたい列を指定する必要がある。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABLES = [0, 1]\n",
    "LABEL_COLUMN = \"survived\"\n",
    "\n",
    "FEATURE_COLUMNS = [column for column in CSV_COLUMNS if column != LABEL_COLUMN]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "コンストラクタの引数の値が揃ったので、ファイルからCSVデータを読み込んでデータセットを作る。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(file_path):\n",
    "    dataset = tf.data.experimental.make_csv_dataset(\n",
    "        file_path,\n",
    "        batch_size=12,\n",
    "        label_name=LABEL_COLUMN,\n",
    "        na_value='?',\n",
    "        num_epochs=1,\n",
    "        ignore_errors=True\n",
    "    )\n",
    "    return dataset\n",
    "\n",
    "raw_train_data = get_dataset(train_file_path)\n",
    "raw_test_data = get_dataset(test_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データセットを構成する要素は、(複数のサンプル, 複数のラベル)の形のタプルとして表されるバッチになっている。\n",
    "サンプル中のデータは(行ベースのテンソルではなく)列ベースのテンソルとして構成され、それぞれはバッチサイズ(このケースでは12個)の要素が含まれる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "EXAMPLES: \n OrderedDict([('sex', <tf.Tensor: shape=(12,), dtype=string, numpy=\narray([b'female', b'male', b'male', b'male', b'female', b'male', b'male',\n       b'female', b'male', b'female', b'male', b'female'], dtype=object)>), ('age', <tf.Tensor: shape=(12,), dtype=float32, numpy=\narray([38.  , 28.  , 48.  , 19.  , 58.  , 28.  , 28.  , 28.  , 43.  ,\n       14.  ,  0.83, 24.  ], dtype=float32)>), ('n_siblings_spouses', <tf.Tensor: shape=(12,), dtype=int32, numpy=array([1, 8, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0])>), ('parch', <tf.Tensor: shape=(12,), dtype=int32, numpy=array([0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0])>), ('fare', <tf.Tensor: shape=(12,), dtype=float32, numpy=\narray([ 71.283,  69.55 ,   7.854,  10.171, 146.521,   7.725,   8.05 ,\n         7.75 ,   8.05 ,  30.071,  29.   ,  13.   ], dtype=float32)>), ('class', <tf.Tensor: shape=(12,), dtype=string, numpy=\narray([b'First', b'Third', b'Third', b'Third', b'First', b'Third',\n       b'Third', b'Third', b'Third', b'Second', b'Second', b'Second'],\n      dtype=object)>), ('deck', <tf.Tensor: shape=(12,), dtype=string, numpy=\narray([b'C', b'unknown', b'unknown', b'unknown', b'B', b'unknown',\n       b'unknown', b'unknown', b'unknown', b'unknown', b'unknown',\n       b'unknown'], dtype=object)>), ('embark_town', <tf.Tensor: shape=(12,), dtype=string, numpy=\narray([b'Cherbourg', b'Southampton', b'Southampton', b'Southampton',\n       b'Cherbourg', b'Queenstown', b'Southampton', b'Queenstown',\n       b'Southampton', b'Cherbourg', b'Southampton', b'Southampton'],\n      dtype=object)>), ('alone', <tf.Tensor: shape=(12,), dtype=string, numpy=\narray([b'n', b'n', b'y', b'y', b'y', b'y', b'y', b'y', b'y', b'n', b'n',\n       b'y'], dtype=object)>)]) \n\nLABELS: \n tf.Tensor([1 0 0 0 1 0 0 1 0 1 1 0], shape=(12,), dtype=int32)\n"
    }
   ],
   "source": [
    "examples, labels = next(iter(raw_train_data))\n",
    "print(\"EXAMPLES: \\n\", examples, '\\n')\n",
    "print(\"LABELS: \\n\", labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データの前処理\n",
    "\n",
    "### カテゴリデータ\n",
    "\n",
    "このCSVデータ中のいくつかの列はカテゴリ列になっている。つまり、その中身は、限られた選択肢の中のひとつである必要がある。\n",
    "\n",
    "このCSVでは、これらの選択肢はテキストとして表現されている。このテキストは、モデルの訓練を行えるように数字に変換する必要がある。これをやりやすくするため、カテゴリ列のリストとその選択肢のリストを作成する必要がある。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "CATEGORIES = {\n",
    "    \"sex\": [\"male\", \"female\"],\n",
    "    \"class\": [\"First\", \"Second\", \"Third\"],\n",
    "    \"deck\": ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J'],\n",
    "    \"embark_town\": [\"Cherbourg\", \"Southhampton\", \"Queenstown\"],\n",
    "    \"alone\": ['y', 'n']\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "カテゴリ値のテンソルを受け取り、それを名前のリストとマッチングして、さらにワンホット・エンコーディングを行う関数を書く"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_categorical_data(data, categories):\n",
    "    \"\"\"カテゴリ値を表すワンホット・エンコーディングされたテンソルを返す\"\"\"\n",
    "\n",
    "    # 最初の' 'を取り除く\n",
    "    data = tf.strings.regex_replace(data, '^ ', '')\n",
    "\n",
    "    # 最後の'.'を取り除く\n",
    "    data = tf.strings.regex_replace(data, r'\\.$', '')\n",
    "\n",
    "    # ワンホット・エンコーディング\n",
    "    # dataを1次元(リスト)から2次元(要素が1個のリストのリスト)にreshape\n",
    "    data = tf.reshape(data, [-1, 1])\n",
    "    # それぞれの要素について、カテゴリ数の長さの真偽値のリストで、\n",
    "    # 要素のカテゴリのラベルが一致したところがTrueとなるものを作成\n",
    "    data = categories == data\n",
    "    # 真偽値を浮動小数点数にキャスト\n",
    "    data = tf.cast(data, tf.float32)\n",
    "\n",
    "    # エンコーディング全体を次の1行に収めることもできる。\n",
    "    # data = tf.cast(categories == tf.reshape(data, [-1, 1]), tf.float32)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "この処理を可視化するため、最初のバッチからカテゴリ列のテンソル1つを取り出し、処理を行い、前後の状態を示す。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tf.Tensor: shape=(12,), dtype=string, numpy=\narray([b'First', b'Third', b'Third', b'Third', b'First', b'Third',\n       b'Third', b'Third', b'Third', b'Second', b'Second', b'Second'],\n      dtype=object)>"
     },
     "metadata": {},
     "execution_count": 41
    }
   ],
   "source": [
    "class_tensor = examples[\"class\"]\n",
    "class_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['First', 'Second', 'Third']"
     },
     "metadata": {},
     "execution_count": 42
    }
   ],
   "source": [
    "class_categories = CATEGORIES[\"class\"]\n",
    "class_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tf.Tensor: shape=(12, 3), dtype=float32, numpy=\narray([[1., 0., 0.],\n       [0., 0., 1.],\n       [0., 0., 1.],\n       [0., 0., 1.],\n       [1., 0., 0.],\n       [0., 0., 1.],\n       [0., 0., 1.],\n       [0., 0., 1.],\n       [0., 0., 1.],\n       [0., 1., 0.],\n       [0., 1., 0.],\n       [0., 1., 0.]], dtype=float32)>"
     },
     "metadata": {},
     "execution_count": 43
    }
   ],
   "source": [
    "processed_class = process_categorical_data(class_tensor, class_categories)\n",
    "processed_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Size of batch:  12\nNumber of category labels:  3\nShape of one-hot encoded tensor:  (12, 3)\n"
    }
   ],
   "source": [
    "print(\"Size of batch: \", len(class_tensor.numpy()))\n",
    "print(\"Number of category labels: \", len(class_categories))\n",
    "print(\"Shape of one-hot encoded tensor: \", processed_class.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 連続データ\n",
    "\n",
    "連続データは値が0と1の間になるように標準化する必要がある。これを行うために、それぞれの値を、1を列値の平均の2倍で割ったものを掛ける関数を書く。データの2次元のテンソルへのreshapeも行う。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_continuous_data(data, mean):\n",
    "    # dataの標準化\n",
    "    data = tf.cast(data, tf.float32) * 1/(2*mean)\n",
    "    return tf.reshape(data, [-1, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "この計算を行うためには、列値の平均が必要。今回のチュートリアルでは平均値が下記の通り得られたものとする。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "MEANS = {\n",
    "    \"age\": 29.631308,\n",
    "    \"n_siblings_spouses\": 0.545455,\n",
    "    \"parch\": 0.379585,\n",
    "    \"fare\": 34.385399\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tf.Tensor: shape=(12,), dtype=float32, numpy=\narray([38.  , 28.  , 48.  , 19.  , 58.  , 28.  , 28.  , 28.  , 43.  ,\n       14.  ,  0.83, 24.  ], dtype=float32)>"
     },
     "metadata": {},
     "execution_count": 47
    }
   ],
   "source": [
    "age_tensor = examples[\"age\"]\n",
    "age_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tf.Tensor: shape=(12, 1), dtype=float32, numpy=\narray([[0.641],\n       [0.472],\n       [0.81 ],\n       [0.321],\n       [0.979],\n       [0.472],\n       [0.472],\n       [0.472],\n       [0.726],\n       [0.236],\n       [0.014],\n       [0.405]], dtype=float32)>"
     },
     "metadata": {},
     "execution_count": 48
    }
   ],
   "source": [
    "process_continuous_data(age_tensor, MEANS[\"age\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### データの前処理\n",
    "\n",
    "これらの前処理のタスクを1つの関数にまとめ、データセット内のバッチにマッピングできるようにする。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(features, labels):\n",
    "    # カテゴリ特徴量の処理\n",
    "    for feature in CATEGORIES.keys():\n",
    "        features[feature] = process_categorical_data(features[feature],\n",
    "                                                        CATEGORIES[feature])\n",
    "    # 連続特徴量の処理\n",
    "    for feature in MEANS.keys():\n",
    "        features[feature] = process_continuous_data(features[feature],\n",
    "                                                       MEANS[feature])\n",
    "\n",
    "    # 特徴量を1つのテンソルに組み立てる\n",
    "    features = tf.concat([features[column] for column in FEATURE_COLUMNS], 1)\n",
    "\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次に、`tf.Dataset.map`関数を使って適用し、過学習を防ぐためにデータセットをシャッフルする。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = raw_train_data.map(preprocess).shuffle(500)\n",
    "test_data = raw_test_data.map(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(<tf.Tensor: shape=(12, 24), dtype=float32, numpy=\n array([[1.   , 0.   , 0.472, 0.   , 0.   , 0.103, 0.   , 0.   , 1.   ,\n         0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   ,\n         0.   , 0.   , 0.   , 0.   , 1.   , 0.   ],\n        [0.   , 1.   , 0.523, 0.   , 0.   , 0.114, 0.   , 0.   , 1.   ,\n         0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   ,\n         0.   , 0.   , 0.   , 0.   , 1.   , 0.   ],\n        [1.   , 0.   , 0.472, 0.   , 0.   , 0.115, 0.   , 0.   , 1.   ,\n         0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   ,\n         0.   , 0.   , 0.   , 0.   , 1.   , 0.   ],\n        [1.   , 0.   , 0.253, 0.917, 1.317, 0.105, 0.   , 0.   , 1.   ,\n         0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   ,\n         0.   , 1.   , 0.   , 0.   , 0.   , 1.   ],\n        [1.   , 0.   , 0.776, 0.   , 0.   , 1.152, 1.   , 0.   , 0.   ,\n         0.   , 1.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   ,\n         0.   , 1.   , 0.   , 0.   , 1.   , 0.   ],\n        [0.   , 1.   , 0.405, 0.   , 0.   , 0.189, 0.   , 1.   , 0.   ,\n         0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   ,\n         0.   , 0.   , 0.   , 0.   , 1.   , 0.   ],\n        [1.   , 0.   , 0.54 , 0.   , 0.   , 0.122, 0.   , 0.   , 1.   ,\n         0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   ,\n         0.   , 0.   , 0.   , 0.   , 1.   , 0.   ],\n        [1.   , 0.   , 0.354, 0.   , 0.   , 0.113, 0.   , 0.   , 1.   ,\n         0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   ,\n         0.   , 0.   , 0.   , 0.   , 1.   , 0.   ],\n        [1.   , 0.   , 0.388, 0.   , 1.317, 0.921, 1.   , 0.   , 0.   ,\n         0.   , 0.   , 0.   , 1.   , 0.   , 0.   , 0.   , 0.   , 0.   ,\n         0.   , 1.   , 0.   , 0.   , 0.   , 1.   ],\n        [1.   , 0.   , 0.472, 0.   , 0.   , 0.378, 1.   , 0.   , 0.   ,\n         1.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   ,\n         0.   , 0.   , 0.   , 0.   , 1.   , 0.   ],\n        [1.   , 0.   , 0.793, 0.   , 0.   , 0.105, 0.   , 0.   , 1.   ,\n         0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   ,\n         0.   , 0.   , 0.   , 0.   , 1.   , 0.   ],\n        [1.   , 0.   , 0.472, 0.   , 0.   , 0.117, 0.   , 0.   , 1.   ,\n         0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   ,\n         0.   , 0.   , 0.   , 0.   , 1.   , 0.   ]], dtype=float32)>,\n <tf.Tensor: shape=(12,), dtype=int32, numpy=array([0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0])>)"
     },
     "metadata": {},
     "execution_count": 51
    }
   ],
   "source": [
    "examples, labels = next(iter(train_data))\n",
    "examples, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデルの構築\n",
    "\n",
    "この例では、Keras Functional APIを使用し、単純なモデルを構築するために`get_model`コンストラクタでラッピングしている。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(input_dim, hidden_units=[100]):\n",
    "    \"\"\"複数の層を持つ Keras モデルを作成\n",
    "\n",
    "    引数:\n",
    "        input_dim: (int) バッチ中のアイテムの形状\n",
    "        labels_dim: (int) ラベルの形状\n",
    "        hidden_units: [int] DNN の層のサイズ（入力層が先）\n",
    "        learning_rate: (float) オプティマイザの学習率\n",
    "    戻り値:\n",
    "        Keras モデル\n",
    "    \"\"\"\n",
    "    inputs = tf.keras.Input(shape=(input_dim,))\n",
    "    x = inputs\n",
    "\n",
    "    for units in hidden_units:\n",
    "        x = tf.keras.layers.Dense(units, activation=\"relu\")(x)\n",
    "    outputs = tf.keras.layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape, output_shape = train_data.output_shapes\n",
    "input_dimension = input_shape.dims[1] # [0]はバッチサイズ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 訓練、評価、そして予測"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Epoch 1/20\n53/53 [==============================] - 1s 12ms/step - loss: 0.6026 - accuracy: 0.7193\nEpoch 2/20\n53/53 [==============================] - 0s 3ms/step - loss: 0.4999 - accuracy: 0.7847\nEpoch 3/20\n53/53 [==============================] - 0s 3ms/step - loss: 0.4533 - accuracy: 0.8086\nEpoch 4/20\n53/53 [==============================] - 0s 3ms/step - loss: 0.4328 - accuracy: 0.8086\nEpoch 5/20\n53/53 [==============================] - 0s 3ms/step - loss: 0.4235 - accuracy: 0.8198\nEpoch 6/20\n53/53 [==============================] - 0s 3ms/step - loss: 0.4176 - accuracy: 0.8230\nEpoch 7/20\n53/53 [==============================] - 0s 3ms/step - loss: 0.4148 - accuracy: 0.8246\nEpoch 8/20\n53/53 [==============================] - 0s 3ms/step - loss: 0.4172 - accuracy: 0.8214\nEpoch 9/20\n53/53 [==============================] - 0s 3ms/step - loss: 0.4048 - accuracy: 0.8309\nEpoch 10/20\n53/53 [==============================] - 0s 3ms/step - loss: 0.4060 - accuracy: 0.8262\nEpoch 11/20\n53/53 [==============================] - 0s 3ms/step - loss: 0.3984 - accuracy: 0.8246\nEpoch 12/20\n53/53 [==============================] - 0s 3ms/step - loss: 0.4003 - accuracy: 0.8341\nEpoch 13/20\n53/53 [==============================] - 0s 3ms/step - loss: 0.3947 - accuracy: 0.8262\nEpoch 14/20\n53/53 [==============================] - 0s 3ms/step - loss: 0.3944 - accuracy: 0.8278\nEpoch 15/20\n53/53 [==============================] - 0s 3ms/step - loss: 0.3923 - accuracy: 0.8293\nEpoch 16/20\n53/53 [==============================] - 0s 3ms/step - loss: 0.3862 - accuracy: 0.8293\nEpoch 17/20\n53/53 [==============================] - 0s 3ms/step - loss: 0.3857 - accuracy: 0.8357\nEpoch 18/20\n53/53 [==============================] - 0s 3ms/step - loss: 0.3852 - accuracy: 0.8405\nEpoch 19/20\n53/53 [==============================] - 0s 3ms/step - loss: 0.3823 - accuracy: 0.8357\nEpoch 20/20\n53/53 [==============================] - 0s 3ms/step - loss: 0.3813 - accuracy: 0.8373\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tensorflow.python.keras.callbacks.History at 0x2b75e80e648>"
     },
     "metadata": {},
     "execution_count": 54
    }
   ],
   "source": [
    "model = get_model(input_dimension)\n",
    "model.compile(\n",
    "    loss=\"binary_crossentropy\",\n",
    "    optimizer=\"adam\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "model.fit(train_data, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "22/Unknown - 0s 6ms/step - loss: 0.4395 - accuracy: 0.8068\n\nTest Loss 0.43948988210071216, Test Accuracy 0.8068181872367859\n"
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(test_data)\n",
    "print(\"\\n\\nTest Loss {}, Test Accuracy {}\".format(test_loss, test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "単一のバッチ、または、バッチからなるデータセットのラベルを推論する場合には、`tf.keras.Model.predict`を使う"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Predicted survival: 90.75%  | Actual outcome:  DIED\nPredicted survival: 44.78%  | Actual outcome:  SURVIVED\nPredicted survival: 19.82%  | Actual outcome:  DIED\nPredicted survival: 11.56%  | Actual outcome:  DIED\nPredicted survival: 93.40%  | Actual outcome:  DIED\nPredicted survival: 64.14%  | Actual outcome:  DIED\nPredicted survival: 10.08%  | Actual outcome:  DIED\nPredicted survival: 48.89%  | Actual outcome:  SURVIVED\nPredicted survival: 11.22%  | Actual outcome:  SURVIVED\nPredicted survival: 6.41%  | Actual outcome:  DIED\n"
    }
   ],
   "source": [
    "predictions = model.predict(test_data)\n",
    "\n",
    "# 結果のいくつかを表示\n",
    "for prediction, survived in zip(predictions[:10], list(test_data)[0][1][:10]):\n",
    "    print(\"Predicted survival: {:.2%}\".format(prediction[0]),\n",
    "          \" | Actual outcome: \",\n",
    "          (\"SURVIVED\" if bool(survived) else \"DIED\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1594347566148",
   "display_name": "Python 3.7.7 64-bit ('tensorflow2': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}