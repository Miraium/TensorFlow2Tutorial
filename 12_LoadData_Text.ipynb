{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1595257998772",
   "display_name": "Python 3.7.7 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf.dataを使ったテキストの読み込み\n",
    "\n",
    "このチュートリアルでは、`tf.data.TextLineDataset`を使ってテキストファイルからサンプルを読み込む方法を例示する。\n",
    "`TextLineDataset`は、テキストファイルからデータセットを作成するために設計されている。この中では、元のテキストファイルの一行一行がサンプルになっている。\n",
    "これは、（例えば、詩やエラーログのような）基本的に行ベースのテキストデータを扱うのに便利。\n",
    "\n",
    "このチュートリアルでは、同じ作品であるホーマーのイリアッドの異なる３つの英語翻訳版を使い、テキスト１行から翻訳者を特定するモデルを構築する。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3つの翻訳のテキストは次のとおり。\n",
    "\n",
    "- [William Cowper](https://en.wikipedia.org/wiki/William_Cowper) - [text](https://storage.googleapis.com/download.tensorflow.org/data/illiad/cowper.txt)\n",
    "- [Edward, Earl of Derby](https://en.wikipedia.org/wiki/Edward_Smith-Stanley,_14th_Earl_of_Derby) - [text](https://storage.googleapis.com/download.tensorflow.org/data/illiad/derby.txt)\n",
    "- [Samuel Butler](https://en.wikipedia.org/wiki/Samuel_Butler_%28novelist%29) - [text](https://storage.googleapis.com/download.tensorflow.org/data/illiad/butler.txt)\n",
    "\n",
    "このチュートリアルで使われているテキストファイルは、ヘッダ、フッタ、行番号、章のタイトルの削除など、いくつかの典型的な前処理を行ったもの。前処理後のファイルをダウンロードする。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'C:\\\\Users\\\\ognek\\\\.keras\\\\datasets'"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "DIRECTORY_URL = 'https://storage.googleapis.com/download.tensorflow.org/data/illiad/'\n",
    "FILE_NAMES = ['cowper.txt', 'derby.txt', 'butler.txt']\n",
    "\n",
    "for name in FILE_NAMES:\n",
    "    text_dir = tf.keras.utils.get_file(name, origin=DIRECTORY_URL+name)\n",
    "\n",
    "parent_dir = os.path.dirname(text_dir)\n",
    "\n",
    "parent_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## テキストをデータセットに読み込む\n",
    "\n",
    "ファイルをイテレートし、それぞれを別々のデータセットに読み込む。\n",
    "\n",
    "サンプルはそれぞれにラベル付けが必要なので、ラベル付け関数を適用するために`tf.data.Dataset.map`を使う。\n",
    "このメソッドは、データセット無いのすべてのサンプルをイテレートし、(`example, label`)というペアを返す。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "0\n1\n2\n"
    }
   ],
   "source": [
    "def labeler(example, index):\n",
    "    return example, tf.cast(index, tf.int64)\n",
    "\n",
    "labeled_data_sets = []\n",
    "\n",
    "for i, file_name in enumerate(FILE_NAMES):\n",
    "    print(i)\n",
    "    lines_dataset = tf.data.TextLineDataset(os.path.join(parent_dir, file_name))\n",
    "    labeled_dataset = lines_dataset.map(lambda ex: labeler(ex, i))\n",
    "    labeled_data_sets.append(labeled_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ラベル付けの終わったデータセットを結合して一つのデータセットにし、シャッフルする。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 50000\n",
    "BATCH_SIZE = 64\n",
    "TAKE_SIZE = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_labeled_data = labeled_data_sets[0]\n",
    "for labeled_dtaset in labeled_data_sets[1:]:\n",
    "    all_labeled_data = all_labeled_data.concatenate(labeled_dataset)\n",
    "\n",
    "all_labeled_data = all_labeled_data.shuffle(\n",
    "    BUFFER_SIZE, reshuffle_each_iteration=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`tf.data.Dtaset.take`と`print`を使って、`(example, label)`のペアがどのようなものかを見ることができる。\n",
    "`numpy`プロパティがそれぞれのテンソルの値を示す。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(<tf.Tensor: shape=(), dtype=string, numpy=b'Should wage before you the wide-wasting war.'>, <tf.Tensor: shape=(), dtype=int64, numpy=0>)\n(<tf.Tensor: shape=(), dtype=string, numpy=b'necks of the horses Then Juno put her steeds under the yoke, eager for'>, <tf.Tensor: shape=(), dtype=int64, numpy=2>)\n(<tf.Tensor: shape=(), dtype=string, numpy=b'Confusion seized his brain; his noble limbs'>, <tf.Tensor: shape=(), dtype=int64, numpy=0>)\n(<tf.Tensor: shape=(), dtype=string, numpy=b'Should any of the everlasting Gods'>, <tf.Tensor: shape=(), dtype=int64, numpy=0>)\n(<tf.Tensor: shape=(), dtype=string, numpy=b'hold your ground here, and go about among the host to rally them in'>, <tf.Tensor: shape=(), dtype=int64, numpy=2>)\n"
    }
   ],
   "source": [
    "for ex in all_labeled_data.take(5):\n",
    "    print(ex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## テキスト行を数字にエンコードする\n",
    "\n",
    "機械学習モデルが扱うのは単語ではなく数字であるため、文字列は数字のリストに変換する必要がある。\n",
    "このため、一意の単語を一意の数字にマッピングする。\n",
    "\n",
    "### ボキャブラリーの構築\n",
    "\n",
    "まず最初に、テキストをトークン化し、個々の一意な単語の集まりとして、ボキャブラリーを構築します。\n",
    "これを行うには、TensorFlowやPythonを使ういくつかの方法があります。\n",
    "ここでは次のようにする。\n",
    "\n",
    "1. 各サンプルの`numpy`値をイテレートする。\n",
    "2. `tfds.feature.text.Tokenizer`を使って、それをトークンに分割する。\n",
    "3. 重複を排除するため、トークンをPythonの集合に集約する。\n",
    "4. あとで使用するため、ボキャブラリーのサイズを取得する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "14436"
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "tokenizer = tfds.features.text.Tokenizer()\n",
    "\n",
    "vocabulary_set = set()\n",
    "for text_tensor, _ in all_labeled_data:\n",
    "    some_tokens = tokenizer.tokenize(text_tensor.numpy())\n",
    "    vocabulary_set.update(some_tokens)\n",
    "\n",
    "vocab_size = len(vocabulary_set)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### サンプルをエンコードする\n",
    "\n",
    "`vocabulary_set`を`tfds.features.text.TokenTextEncoder`に渡してエンコーダーを作成する。\n",
    "エンコーダーの`encode`メソッドは、テキストを文字列に引数にとり、整数のリストを返す。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = tfds.features.text.TokenTextEncoder(vocabulary_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "b'Should wage before you the wide-wasting war.'\n[9667, 11431, 5190, 12319, 10942, 12689, 3648, 13604]\n<class 'bytes'>\n7\n8\n"
    }
   ],
   "source": [
    "# 1行だけに適用してみて、出力がどの様になるか確認する\n",
    "# テキストが数値に置き換わっていることを確認すること。\n",
    "example_text = next(iter(all_labeled_data))[0].numpy()\n",
    "print(example_text)\n",
    "encoded_example = encoder.encode(example_text)\n",
    "print(encoded_example)\n",
    "\n",
    "# 念の為単語の数が同じ担っていることを確認 (example_textはbyte型なのでstr型に変換してからsplit\n",
    "print(type(example_text))\n",
    "print(len(example_text.decode(\"utf-8\").split(' ')))\n",
    "print(len(encoded_example))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次に、このエンコーダーを`tf.py_function`でラッピングして、データセットの`map`メソッドに渡し、データセットに適用する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(text_tensor, label):\n",
    "    encoded_text = encoder.encode(text_tensor.numpy())\n",
    "    return encoded_text, label\n",
    "\n",
    "def encode_map_fn(text, label):\n",
    "    # py_func は返してくるテンソルのshapeをsetしない\n",
    "    encoded_text, label = tf.py_function(encode,\n",
    "                                         inp=[text, label],\n",
    "                                         Tout=(tf.int64, tf.int64))\n",
    "    # `tf.data.Dataset`はすべての要素がshapeを持っているときに（最良に？）動作するため、\n",
    "    # shapeを手動で与えておく\n",
    "    encoded_text.set_shape([None])\n",
    "    label.set_shape([])\n",
    "\n",
    "    return encoded_text, label\n",
    "\n",
    "all_encoded_data = all_labeled_data.map(encode_map_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データセットを、テスト用と訓練用のバッチに分割する\n",
    "`tf.data.Dataset.take`とtf.data.Dataset.skip`を使って、小さなテスト用データセットを、より大きな訓練用セットを作成する。  \n",
    "\n",
    "モデルに渡す前に、データセットをバッチ化する必要がある。通常、バッチの中のサンプルは、同じサイズと形状である必要がある。しかし、これらのデータセットの中にはのサンプルはすべて同じサイズではない。\n",
    "具体的には、テキストの各行の単語数は異なっている。\n",
    "このため、（`batch`の代わりに）`tf.data.Dataset.padded_batch`メソッドを使ってサンプルを同じサイズにパディングする。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# チュートリアルのとおりにやるとエラーが出る。\n",
    "# padded_batchの第2引数を与えないとエラーが出てしまうバグあるようなので追加。\n",
    "# https://github.com/tensorflow/tensorflow/issues/36308\n",
    "train_data = all_encoded_data.skip(TAKE_SIZE).shuffle(BUFFER_SIZE)\n",
    "train_data = train_data.padded_batch(BATCH_SIZE,  ([None],()))\n",
    "\n",
    "test_data = all_encoded_data.take(TAKE_SIZE)\n",
    "test_data = test_data.padded_batch(BATCH_SIZE,  ([None],()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "この段階で、`test_data`と`train_data`は、(`example, label`)というペアのコレクションではなく、バッチのコレクションになっている。\n",
    "それぞれのバッチは、(*たくさんのサンプル, たくさんのラベル*)という配列のペアになっている。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(<tf.Tensor: shape=(16,), dtype=int64, numpy=\n array([ 9667, 11431,  5190, 12319, 10942, 12689,  3648, 13604,     0,\n            0,     0,     0,     0,     0,     0,     0], dtype=int64)>,\n <tf.Tensor: shape=(), dtype=int64, numpy=0>)"
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "source": [
    "sample_text, sample_labels = next(iter(test_data))\n",
    "sample_text[0], sample_labels[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "（ゼロをパディングに使用したことで）新しいトークン番号を1つ導入したので、ボキャブラリーサイズは1つ増えている。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデルを構築する\n",
    "\n",
    "最初の層は、整数表現を密なベクトル埋め込みに変換する。詳細は\n",
    "[単語埋め込み](https://www.tensorflow.org/tutorials/text/word_embeddings?hl=ja)\n",
    "のチュートリアルを参照。\n",
    "\n",
    "次の層は、Long Short-Term Memory層(LSTM層)とする。この層により、モデルは単語を他の単語の文脈の中で解釈する。\n",
    "LSTMのBidirectionalラッパーにより、データポイントを、その前とその後のデータポイントとの関連で学習することができる。\n",
    "\n",
    "続いての層は、1つ以上の全結合層がいくつかあるとして、最終層が出力層となる。最終層はラベルすべての確率を生成する。\n",
    "最も確率の高いラベルが、モデルが余録するサンプルのラベルとなる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Embedding(vocab_size, 64))\n",
    "model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)))\n",
    "\n",
    "# 1つ以上の全結合層を追加。今回は64unitの層を2つ入れている。\n",
    "# 別の値をいれて実験してもよい\n",
    "for units in [64, 64]:\n",
    "    model.add(tf.keras.layers.Dense(units, activation=\"relu\"))\n",
    "\n",
    "# 出力層 最初の引数はラベルの数 (= 3人のうちどの翻訳者かを推定するので、3)\n",
    "model.add(tf.keras.layers.Dense(3, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最後似モデルをコンパイルする。softmaxによるカテゴリー分類モデルでは、損失関数として`sparse_categorical_crossentropy`を使用する。\n",
    "他のoptimizerを使うこともできるが、optimizerには`adam`がよく使われる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"adam\",\n",
    "              loss=\"sparse_categorical_crossentropy\",\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデルを訓練する\n",
    "\n",
    "このモデルをこのデータに適用すると、約83%のまともな結果が得られる。\n",
    "（実際に動かしてみると99%も出てしまう。。高すぎ？）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Epoch 1/3\n601/601 [==============================] - 16s 26ms/step - loss: 0.0041 - accuracy: 0.9986 - val_loss: 0.0439 - val_accuracy: 0.9880\nEpoch 2/3\n601/601 [==============================] - 14s 24ms/step - loss: 0.0025 - accuracy: 0.9992 - val_loss: 0.0404 - val_accuracy: 0.9912\nEpoch 3/3\n601/601 [==============================] - 14s 23ms/step - loss: 0.0016 - accuracy: 0.9996 - val_loss: 0.0302 - val_accuracy: 0.9930\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tensorflow.python.keras.callbacks.History at 0x1eb5401bc08>"
     },
     "metadata": {},
     "execution_count": 32
    }
   ],
   "source": [
    "model.fit(train_data, epochs=3, validation_data=test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "79/Unknown - 2s 23ms/step - loss: 0.0302 - accuracy: 0.9930\nEval loss: 0.030177595910940967, Eval accuracy: 0.9929999709129333\n"
    }
   ],
   "source": [
    "eval_loss, eval_acc = model.evaluate(test_data)\n",
    "print(\"\\nEval loss: {}, Eval accuracy: {}\".format(eval_loss, eval_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}